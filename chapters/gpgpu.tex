e\chapter{GPGPU}
\label{cap:gpgpu}

A seguir é explanado o que vem a ser uma placa gráfica, principalmente seu processador, a GPU. Depois é descrita a motivação para a qual esse hardware foi criado, o por quê ainda é produzido e por que provavelmente continuará. Seguindo são apontadas as diferenças arquiteturais entre uma CPU e uma GPU, vantagens e desvatagens. Na continuação são apresentadas e demonstradas as duas principais linguagens que permitem o acesso a programação genérica na GPGPU a OpenCL e a CUDA, contendo o por quê neste projeto foi escolhido a linguagem CUDA. Por fim são apresentadas aplicações que se utilizam do estado da arte no hardware e software das placas gráficas mais atuais e avançadas.

\section{Placas de vídeo}
\label{sec:video_boards}
  \begin{figure}[!h]
    \centering
    \includegraphics[width=.80\textwidth]{NVIDIA-GTX-1070-FoundersEdition-FL.jpg}
    \caption{Placa grafica NVIDIA GTX 1070 para computadores de mesa "desktop", essa versão é a "Founders edition", possui cerca de 8 GBs de memória com uma banda de 256 GB/s , 1920 núcleos, frequência de funcionamento de 1506 MHz e consome 150W. (Fonte: Wikimedia Commons \protect\footnotemark)}
    \label{fig:gtx1070}
  \end{figure}

  \footnotetext{Disponível em \url{https://commons.wikimedia.org/wiki/File:NVIDIA-GTX-1070-FoundersEdition-FL.jpg} Domínio público}

  Placa de vídeo é uma peça que pode ou não estar presente em um computador, ela é responsável por fornecer o hardware especializado em renderização gráfica, projetado para prover um grande aumento de performance a um baixo custo se comparado a CPUs no que disrespeito a tranformação de dados na memoria em imagens e vídeos, prontos para serem apresentados em telas e monitores. São constituídas principalmente de:
  \begin{itemize}
    \item Um chip de memória, cuja a principal função é armazenar texturas, ou qualquer outro dado que deverá ser várias vezes utilizado, consultado, no processo de renderização.
    \item Portas de acesso e conexão a monitores e telas, variam entre VGA (Video Graphics Array), DVI (Digital Visual Interface) e HDMI (High-Definition Muiltimidia Interface). São as saídas mais comuns às computações das placas.
    \item Um processador, uma GPGPU composta de alguns milhares de núcleos
    \item Uma ou várias ventoinhas, para dissipar o calor produzido pelo processador
  \end{itemize}

  A figura \ref{fig:gtx1070} é uma foto de uma placa gráfica da Nvidia, a NVIDIA GTX 1070, começou a ser comercializada em Junho de 2016 à um preço de 379 dólares e possuí 1920 núcleos de processamento à 1,506 GHz em sua GPU \citep{gtx1070:16}. Nessa mesma época também é comercializada o \textit{Intel® Core™ i7-6850K Processor} CPU da Intel, vendida na época a partir de 617 dólares, possui 6 núcleos à uma frequência de 3,6 GHz \citep{inteli7:16}. Um cálculo simples de tiques por segundo, ou seja, quantidade de ciclos que podem ser executados por segundo para cada processador demonstra que, enquanto a CPU da intel é capaz de realizar 21,6 bilhões de ciclos, no total, por segundo a GPGPU da nvidia é capaz de 2891,52 bilhões de ciclos, no total, por segundo, uma diferença de aproximadamente 133,8 vezes. A GPU que é 60,42\% do valor da CPU é mais de 100 vezes mais rápida.

  GPGPU é uma acrônimo para \textit{General Purpose Graphics Processing Unit} em tradução literal: Unidade de processamento Gráfico de Propósito Geral. É uma evolução, uma adaptação que as GPUs passaram, na qual sua cadeia de processamento gráfico foi flexibilizada tornado possível usá-la para propósitos mais gerais, indo muito além do escopo de produção de gráficos e imagens em três dimensões, porém suas origens e modo de operação advem da sua função original de processar uma cadeia de dados gráficos, e tal necessidade determinou qual seria o objetivo da arquitetura que tais processadores deveriam ter. Assim entender essa cadeia de processamento ou \textit{pipeline} dos dados das placas gráficas é importante para entender o por quê são como são \citep{massively:16}.

  O objetivo das GPUs é gerar imagens e vídeos que representam visões de uma cena virtual. A visão desta cena é descrita pela posição de uma câmera virtual e é definida pela geometria, orientação e propriedades materiais da superfície dos objetos na cena representados, bem como das propiedades das fontes de luz. APIs gráficas como OpenGL \citep{opengl}, DirectX \citep{directx} ou Vulkan\citep{vulkan} representam este processo como um pipeline que executa uma série de operações sobre um conjunto de vértices enviados pela CPU, sendo que cada vértice possui algumas propriedades, tais como cor, posição e vetor normal \citep{closer-look:08}.

  O vertex shader é um programa que executa um conjunto de operações para cada um dos vertíces de entrada, com o intuíto de projetar cada vértice, baseado em sua posição relativa à camera virtual, em um espaço de tela 2D. Destes vértices, é montado um conjunto de triângulos, que representam os objetos no espaço 2D. Assim, quanto maior a quantidade de triângulos, melhor a qualidade com que o objeto será representado \citep{closer-look:08}. Visto que cada cena  possui milhares de vértices e cada um deles pode ser tratado independentemente, os vértices podem ser processados paralelamente \citep{gpu-comp:08}.

  A seguir, ocorre o processo de rasterização, que consiste em determinar quais espaços da tela são cobertos por cada triângulo. Esse processo resulta na geração de fragmentos para cada espaço de tela coberto. Um fragmento é o que virá a ser um pixel, que contem todas as informações necessárias para gerar um na imagem final (profundidade, localização no frame buffer, etc.). A partir da posição da câmera virtual, os fragmentos que são ocultos por outros fragmentos são descartados, só é necessário mostras os objetos que podem ser vistos \citep{gpu-comp:08}.

  Já o pixel shader opera sobre a saída gerada pelo processo de rasterização. O pixel shader é um programa que consiste em um conjunto de operações que são executadas sobre cada fragmento, antes que estes sejam plotados na tela. Utilizando as informações de cor dos vértices e, possivelmente, buscando dados adicionais na memória global em forma de texturas (é nesse momento que a memória principal da placa é necessária), cada fragmento é processado para obter-se a cor final do pixel. Assim como na etapa de processamento de vértices, os fragmentos são independentes e podem ser processados em paralelo. Esta etapa é a que tipicamente demanda maior processamento dentro da estrutura do pipeline gráfico \citep{gpu-comp:08}.

  Uma vez que os programas de shader necessitam ser aplicados em milhares de vértices e pixels independentemente, as GPUs evoluíram para um conjunto de multiprocessadores massivamente paralelos. Além disso, dependendo do balanceamento da carga de trabalho da aplicação, apesar dos pixels serem dependentes dos vértices, ambos podem ser executados paralelamente. Esta característica resultou no aumento da programabilidade dos multiprocessadores da GPU \citep{massively:16}.

  Essa cadeia de processamento é o que destacou as GPUs das CPUs convencionais. No desenvolvimento de processadores dedicados a processar essa cadeia na forma mais eficiente possível as GPUs tomaram um caminho mais focado na paralelização, com mais núcleos, mais unidades logico-aritméticas em detrerimento de mais ciclos por segundo, priorizando uma única memória grande e não muito rápida ao invés de varias camadas de memória cache muito rápida.

  Esta arquiterura muito focada, ganhou a atenção de outras computações não necessariamente ligadas a renderização de imagens. Tal demanda eclodiu na flexibilização das GPUs para GPGPUs.


  intel graphycs



\section{A História das GPU e GPGPU}
  As GPUs, a princípio, não foram criadas para cálculos numéricos ou simulações de particulas e sim para a rederização de imagens para jogos digitais, o alto custo na produção e aquisição de hardware mais potente, pricipalmente a memória, na década de 70 impulsionou a criação de processadores mais dedicados e com funções mais específicas para a rederização gráfica. Impulsionada pelo mercado de jogos de video game, e buscando evitar o alto valor de chips de memória, placas de sistemas de arcade apresentavam uma composição de chips de vídeo para combinar os dados enquanto estes eram escaneados saindo para o monitor \citep{Hague:13}.

  Em 1977 o Atari 2600 usou um \textit{video shifter}, um tipo de circuíto integrado responsável por emitir o sinal de TV, chamado \textit{Television Interface Adaptor} ou TIA. Foi customizado para ser capaz de gerar as imagens finais para a tela, os efeitos sonoros e ler os comandos vindos dos joysticks, teve como direcionamento em seu design a economia de memória RAM, muito cara na época \citep{Hague:13} \citep{atari-field:83}.

  Em 1985 o Commodore Amiga apresentou um chip gráfico que vinha com um circuíto \textit{blitter}, para maior velocidade na manipulação de memória, tranformação de bitmaps, desenho de linhas e funções de preenchimento de áreas. Também vinha com um co-pocessador, capaz de executar instruções únicas e manipular os registradores em sincronia com o canhão de desenho dos tubos de raios catódicos das televisões \citep{amiga:17}.

  Na decada de 90 a nintendo e a sony criaram seus consoles de vídeo game, o nintendo 64 e o playstation, ambos capazes de produzir gráficos em 3 dimensões a partir de polígonos. A distição entre os dois principais processadores desses vídeo games eram claras, a nintendo já havia detalhado que seu sistema vinha com dois processadores: um de propositos mais geral o \textit{MIPS R4200} e o \textit{Reality Coprocessor} desenhado para cálculos em 3d de alta performance, pre-processamento de aúdio e vídeo, mapeamento de textura e buferização de profundidade em tempo real \citep{manual64:98} \citep{N64Launch:96}.

  A nintendo não utilizou o termo CPU para descrever seu \textit{Reality Coprocessor}, foi a Nvidia em 1999 que popularizou o termo. Ele já existia desde década de 80, porém somente ápos a capanha de marketing de sua placa de vídeo, a GeForce 256 com o logo: \textit{"the world's first GPU"} (A primeira GPU do mundo) que o termo ficou mais comum \citep{nvida256}.

  Já nos anos 2000 teve mais NVIDIA e ATI
  By October 2002, with the introduction of the ATI Radeon 9700 (also known as R300), the world's first Direct3D 9.0 accelerator, pixel and vertex shaders could implement looping and lengthy floating point math, and were quickly becoming as flexible as CPUs, yet orders of magnitude faster for image-array operations. Pixel shading is often used for bump mapping, which adds texture, to make an object look shiny, dull, rough, or even round or extruded.[37]

  Nos dia atuais:
  The PS4 and Xbox One were released in 2013, they both use GPUs based on AMD's Radeon HD 7850 and 7790. Nvidia's Kepler line of GPUs was followed by the Maxwell line, manufactured on the same process. 28 nm chips by Nvidia were manufactured by TSMC, the Taiwan Semiconductor Manufacturing Company, that was manufacturing using the 28 nm process at the time. Compared to the 40 nm technology from the past, this new manufacturing process allowed a 20 percent boost in performance while drawing less power.[48][49] Virtual reality headsets like the Oculus Rift and the HTC Vive have very high system requirements. Headset manufacturers have recommended GPUs for good virtual reality experiences. At their release, they had the GTX 970 from Nvidia and the R9 290 from AMD as the recommended GPUs.[50][51] Pascal is the newest generation of graphics cards by Nvidia released in 2016. The GeForce 10 series of cards are under this generation of graphics cards. They are made using the 16 nm manufacturing process which improves upon previous microarchitectures.[52] The Polaris 11 and Polaris 10 GPUs from AMD are made with a 14 nm process. Their release results in a big increase in the performance per watt of AMD video cards.[53]


\section{CPU vs GPU}

Em 2009, o poder computacional para operações de ponto flutuante de uma GPU
era cerca de dez vezes superiores ao de uma CPU tradicional, ver seção ~\ref{sec:video_boards}. Já a sua largura de
banda era cerca de três vezes superior (KIRK; HWU, 2010). Para compreender os
motivos desta diferença de desempenho, é necessário entender os propósitos de cada
um desses processadores.
A CPU é otimizada para desempenho de código sequencial, sendo que nessa,
geralmente, utilizam-se de uma lógica de controle mais sofisticada, fazendo com que
a CPU seja projetada com mecanismos como previsão de desvios e execução fora de
ordem. Desta forma, permite-se que um único fluxo de instruções seja executado em
paralelo enquanto a aparência de uma execução sequencial é mantida. Além disso,19
a CPU também é otimizada de forma a reduzir a latência de acesso aos dados e às
instruções na memória principal. De fato, grande parte dos recursos da CPU são
utilizadas para o gerenciamento de vários nı́veis de memória cache, que possuem o
objetivo de minimizar o tempo necessário para que os dados requisitados sejam de
fato utilizados pelo processador. Uma vez que a CPU tem como alvo programas
de propósito geral, poucos de seus recursos são utilizados para processamento de
operações de ponto flutuante (KIRK; HWU, 2010).
A GPU é adequada para a solução de problemas onde o processamento dos
dados pode ser realizado massivamente em paralelo, ou seja, o mesmo programa
é executado simultaneamente em muitos elementos com alta intensidade aritmética
(NVIDIA, 2010a). Por causa disto, as GPUs dedicam a maior parte de seus recursos
ao processamento de dados ao invés de cache de dados e controle de fluxo (KIRK;
HWU, 2010).
Na Figura 2.1, pode-se observar a maneira com que os transistores do hardware
são empregados nos chips da CPU e GPU. Na CPU, grande parte da área do chip é
dedicada à memória cache e lógica de controle. Na GPU, diferentemente da CPU,
a maior parte dos recursos do chip é projetada para processamento massivamente
paralelo de operações de ponto-flutuante. Assim, sua capacidade de processamento
é aumentada consideravelmente (NVIDIA, 2010a).

Na GPU, grande parte dos dados é utilizada uma única vez, sendo que estes
dados precisam ser transferidos da memória principal da GPU para o uso de seus
multiprocessadores. Desta forma, o sistema de memória da GPU é projetado para
maximizar a taxa de transferência de dados (throughput), ou seja, a GPU é projetada
de forma a aumentar a largura de banda, ao invés de minimizar a latência para o
acesso aos dados. Esta grande largura de banda é obtida através do uso de amplos
barramentos e memórias dynamic random access memory (DRAM) do tipo graphics
double data rate (GDDR) (FATAHALIAN; HOUSTON, 2008).
É importante destacar que nem todas as aplicações terão um desempenho melhor
na GPU em relação à CPU. As GPUs são projetadas como engines de processamento20
numérico, e não executam bem os programas sequenciais que as CPUs normalmente
executam. Assim, deve-se utilizar a placa GPU como um co-processador para au-
xiliar a CPU na execução dos programas, de forma que sejam atribuı́das as partes
sequenciais à CPU e as partes com processamento paralelo aritmético à GPU (KIRK;
HWU, 2010).

  \begin{figure}[!h]
    \centering
    \includegraphics[width=.80\textwidth]{comparacao_GPU_CPU.png}
    \caption{Imagem comparando de forma simplifica a arquitetura dos processadores. Do lado esquerdo a arquitetura de uma Unidade de Processamento Central, muito espaço para o cache e a unidade de comtrole. Do lado direto uma unidade de processamento Gráfica, como uma grande região segmentada dedicada a computação aritimética e lógica. (Fonte: Wikimedia Commons \protect\footnotemark)}
    \label{fig:cpuvsgpu}
  \end{figure}

  \footnotetext{Disponível em \url{https://commons.wikimedia.org/wiki/File:Cpu-gpu.svg} Creative Commons Attribution 3.0 Unported}


  Na Figura~\ref{fig:cpuvsgpu} texto texto

  era uma vez...

\section{Bibliotecas: OpenCL e CUDA}
Open Computing Language (OpenCL)[3] é uma framework aberta para progra-
mação genérica para vários processadores, dentre eles GPUs e CPUs. OpenCL
da suporte para sistemas embarcados, sistemas pessoais, corporativos e até HPC.
Ele consegue isso criando uma interface de baixo nível, ou seja, o mais próx-
imo do hardware possível, e mantendo auto desempenho, com uma abstração
portátil. O OpencL também é uma API para controle de aplicações paralelas em
sistemas com processadores heterogêneos. O OpenCL consegue, numa mesma
aplicação, reconhecer vários processadores diferentes dentro de um mesmo com-
putador, e executar códigos distintos entre eles, coordenando os hardwares.
Aqui, como no CUDA, a parte do código executado na CPU é chamada de Host
e o hardware que executa os kernels de Devices . É importante lembrar que
dado essa generalização do OpenCL, é possível que a CPU onde o código do
host esteja executando seja usada para rodar um kernel, e essa CPU passa a ser
um device ao mesmo tempo em que roda o host . Tanto o fato do OpenCL ser
aberto quanto o fato dele não se restringir a um hardware especico fazem dele
a linguagem mais usada para GPGPU fora de GPUs NVIDIA

Compute Unied Device Architecture (CUDA)[2] é uma arquitetura de progra-
mação para GPUs criada pela NVIDIA. Ele adiciona suas diretrizes para as
linguagens C, C++, FORTRAN e Java, permitindo que elas usem a GPU. Esse
trabalho usa o CUDA junto com a linguagem C. A versão 1.0 do CUDA foi
disponibilizada no inicio de 2007. Atualmente só existe um compilador para
CUDA, o nvcc, e ele só da suporte para GPUs NVIDIA.
Para uma função executar na GPU ela precisa ser invocada de um programa
da CPU. Chamamos esse programa de Host e a GPU onde o kernel executará
de Device .
O CUDA implementa um conjunto virtual de instruções e memória, tornando
os programas retroativos. O compilador primeiro compila o código em C para
um intermediário, chamado de PTX, que depois será convertido em linguagem
de máquina. Na conversão do PTX para linguagem de máquina o compilador
verica quais instruções o device suporta e converte o código para usar as in-
struções corretas. Para obter o maior desempenho possível, é importante saber
para qual versão o código nal será compilado, pois na passagem do código de
uma versão maior para uma menor não existe a garantia que o algoritmo seguira
as mesmas instruções, o compilador pode mudar um conjunto de instruções para
outro menos eciente, ou em alguns casos, algumas instruções não existem em
versões mais antigas do hardware.


cuda mais facil de aprender


  o que são e como funcionam
  porque escolhemos cuda?
  colocar código de cada uma

\section{Aplicações e usos avançados}

aprendiozado de máquina, deep learning,
O aprendizado profundo (Deep Learning) é a área de mais rápido crescimento em inteligência artificial, ajudando os computadores a compreender dados, como imagens, som e texto. Usando vários níveis de redes neurais, os computadores agora têm a capacidade de ver, aprender e reagir a situações complexas tão bem quanto ou melhor que os humanos. Isso está levando a uma forma totalmente diferente de pensar sobre seus dados, sua tecnologia, e os produtos e serviços que você oferece.

As soluções atuais de aprendizado profundo contam quase exclusivamente com a computação acelerada pelas placas de vídeo NVIDIA (GPU’s) para treinar e acelerar aplicações desafiadoras, como identificação de imagens, caligrafia e vozes. As GPU’s se destacam em cargas de trabalho paralelas e aceleram as redes neurais em até 10 a 20 vezes, reduzindo cada uma das muitas iterações de treinamento de dados de semanas para dias. De fato, as placas de vídeo têm acelerado o treinamento de redes neurais profundas em 50 vezes em apenas três anos — um ritmo muito mais rápido do que a lei de Moore — com outras 10 vezes previstas para os próximos anos. A inovação em Inteligência Artificial está acontecendo em ritmo acelerado. Atualmente, os computadores não estão só aprendendo, estão pensando por si mesmos. Isso está abrindo oportunidades fantásticas em aplicações como robôs, medicina e carros com direção automática. É possível projetar e implementar rapidamente aplicações de aprendizado profundo para aproveitar esses avanços incríveis.

As empresas atentas ao futuro que se aproxima rapidamente e que estão atentas às transformações, estão adotando o aprendizado profundo para lidar com volumes de dados que crescem exponencialmente, melhorias nos algoritmos de aprendizagem automática e avanços em hardware de computação . Isso as ajuda a encontrar novas formas de explorar a riqueza dos dados a seu alcance para desenvolver novos produtos, serviços e processos — e a criar uma vantagem competitiva inovadora.

Neste link você encontra diversos exemplos de como as GPU’s estão sendo usadas para Deep Learning em reconhecimento de imagens:

simulações climáticas
realidade virtual

irtual reality (VR) is a computer-generated scenario that simulates a realistic experience. The immersive environment can be similar to the real world in order to create a lifelike experience grounded in reality or sci-fi. Augmented reality systems may also be considered a form of VR that layers virtual information over a live camera feed into a headset, or through a smartphone or tablet device.

Current VR technology most commonly uses virtual reality headsets or multi-projected environments, sometimes in combination with physical environments or props, to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual or imaginary environment. A person using virtual reality equipment is able to "look around" the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens.

renderização em tempo real de rostos humanos reais
bitcoinbilhões de ciclos, no total, por segundo
In case you’re not aware of the situation, here’s what’s going on: Cryptocurrency mining requires immense computational power, and miners figured out years ago that the chips in many high-end graphics cards are well-suited to the task. The values of cryptocurrencies such as Ethereum and bitcoin have skyrocketed in recent months — a single bitcoin is currently worth well over 11,000, although that figure is in flux and has been trending downward since a peak above 19,000 last month. This has led people to put together “mining rigs” — after buying multiple consumer graphics cards for the necessary computing power — and then try to turn a profit on their investment by mining for cryptocurrency like Ethereum.

For instance, the Nvidia GeForce GTX 1070’s suggested retail price is 380, but it was selling for upward of 450 last year and is now going for more than 700. Lower-end cards like the GTX 1050 and 1060 weren’t affected initially, but miners have moved on to them as the better cards have sold out, pushing up prices for the cheaper GPUs as well. And, as the value of bitcoin spiked in December, prices rose even further.
